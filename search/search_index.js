var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"<code>juju</code>, from JAX to MAX.","text":"<p><code>juju</code> is a bit of compiler middleware bridging JAX to the world of MAX graphs. It allows:</p> <ul> <li>users to write JAX programs, lower those programs to MAX graphs, and execute those graphs on MAX-supported hardware, including CPUs, GPUs, and (later on), xPUs (whatever MAX supports).</li> <li>users to extend the Python JAX language (the primitives that JAX exposes to write numerical programs) with custom MAX kernels.</li> </ul> <p>Danger, Will Robinson!</p> <p>This package is a proof-of-concept, and really early in development. Simple programs only for now! Tons of JAX primitives are missing lowering rules. It's not yet clear how much of JAX will be fully supported (and how many extensions via MAX kernels will be added).</p> <p>Example:</p> <p>Using <code>juju</code> to transform and execute code with MAX.</p> <pre><code>Traceback (most recent call last):\n  File \"/Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/markdown_exec/formatters/python.py\", line 71, in _run_python\n    exec_python(code, code_block_id, exec_globals)\n  File \"/Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/markdown_exec/formatters/_exec_python.py\", line 8, in exec_python\n    exec(compiled, exec_globals)  # noqa: S102\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"&lt;code block: n1&gt;\", line 2, in &lt;module&gt;\n    from juju import jit\n  File \"/Users/femtomc/Dev/juju/src/juju/__init__.py\", line 13, in &lt;module&gt;\n    from .compiler import gpu_engine, jit, make_max_graph\n  File \"/Users/femtomc/Dev/juju/src/juju/compiler.py\", line 269, in &lt;module&gt;\n    class JITEngine:\n  File \"/Users/femtomc/Dev/juju/src/juju/compiler.py\", line 271, in JITEngine\n    session: engine.InferenceSession = engine.InferenceSession(\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/max/engine/api.py\", line 694, in __init__\n    self._impl = _InferenceSession(config)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: Error locating python dependencies: KeyboardInterrupt: &lt;EMPTY MESSAGE&gt;\n\nAt:\n  &lt;frozen importlib.util&gt;(70): find_spec\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/max/engine/api.py(694): __init__\n  /Users/femtomc/Dev/juju/src/juju/compiler.py(271): JITEngine\n  /Users/femtomc/Dev/juju/src/juju/compiler.py(269): &lt;module&gt;\n  &lt;frozen importlib._bootstrap&gt;(488): _call_with_frames_removed\n  &lt;frozen importlib._bootstrap_external&gt;(999): exec_module\n  &lt;frozen importlib._bootstrap&gt;(935): _load_unlocked\n  &lt;frozen importlib._bootstrap&gt;(1331): _find_and_load_unlocked\n  &lt;frozen importlib._bootstrap&gt;(1360): _find_and_load\n  /Users/femtomc/Dev/juju/src/juju/__init__.py(13): &lt;module&gt;\n  &lt;frozen importlib._bootstrap&gt;(488): _call_with_frames_removed\n  &lt;frozen importlib._bootstrap_external&gt;(999): exec_module\n  &lt;frozen importlib._bootstrap&gt;(935): _load_unlocked\n  &lt;frozen importlib._bootstrap&gt;(1331): _find_and_load_unlocked\n  &lt;frozen importlib._bootstrap&gt;(1360): _find_and_load\n  &lt;code block: n1&gt;(2): &lt;module&gt;\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/markdown_exec/formatters/_exec_python.py(8): exec_python\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/markdown_exec/formatters/python.py(71): _run_python\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/markdown_exec/formatters/base.py(144): base_format\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/markdown_exec/formatters/python.py(85): _format_python\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/markdown_exec/__init__.py(127): formatter\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/pymdownx/superfences.py(209): _formatter\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/pymdownx/superfences.py(479): process_nested_block\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/pymdownx/superfences.py(430): eval_fence\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/pymdownx/superfences.py(722): search_nested\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/pymdownx/superfences.py(888): run\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/markdown/core.py(354): convert\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/mkdocs/structure/pages.py(285): render\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/mkdocs/commands/build.py(167): _populate_page\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/mkdocs/commands/build.py(310): build\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/mkdocs/__main__.py(315): gh_deploy_command\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/click/core.py(788): invoke\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/click/core.py(1443): invoke\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/click/core.py(1697): invoke\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/click/core.py(1082): main\n  /Users/femtomc/Dev/juju/.magic/envs/docs/lib/python3.12/site-packages/click/core.py(1161): __call__\n  /Users/femtomc/Dev/juju/.magic/envs/docs/bin/mkdocs(8): &lt;module&gt;\n</code></pre>"},{"location":"index.html#getting-started","title":"Getting started","text":"<p>To get started with <code>juju</code>, you'll need to follow these steps:</p> <ul> <li>First, install <code>magic</code>, the package and compiler manager for MAX and Mojo.</li> <li>Then, clone this repository, and run <code>magic install</code> at the toplevel. This will setup your environment, which you can access via <code>magic shell</code>. </li> <li>Then, run <code>magic run kernels</code> to build the custom MAX kernels provided as part of <code>juju</code>.</li> </ul>"},{"location":"index.html#basic-apis","title":"Basic APIs","text":"<p>To start out, let's examine basic APIs which allow you to execute functions using MAX, and create MAX graphs. </p>"},{"location":"index.html#juju.jit","title":"juju.jit","text":"<pre><code>jit(\n    f: Optional[Callable[..., any]] = None,\n    coerces_to_jnp: bool = False,\n    engine: JITEngine = cpu_engine(),\n)\n</code></pre> <p>Returns a function which JIT compiles the provided function using MAX by first creating a MAX graph, loading it into the MAX engine, and then executing it.</p> <p>The first invocation of the JIT'd function will be slow to compile, but subsequent invocations will be fast, as the graph is cached by MAX, and <code>juju</code> stores a callable function which avoids repeating the lowering process.</p> <p>Example:</p> <pre><code>import jax.numpy as jnp\nfrom juju import jit\n\n\n@jit\ndef foo(x):\n    return x * x\n\n\nprint(foo(5).to_numpy())\n</code></pre> <p>25</p> <p><code>juju.jit</code> supports an option called <code>coerces_to_jnp</code> which can be used to automatically convert MAX tensors to JAX numpy arrays. By default, this option is set to <code>False</code>.</p> <pre><code>import jax.numpy as jnp\nfrom juju import jit\n\n\n@jit(coerces_to_jnp=True)\ndef foo(x):\n    return x * x\n\n\nprint(foo(5))\n</code></pre> <p>25</p> Source code in <code>src/juju/compiler.py</code> <pre><code>def jit(\n    f: Optional[Callable[..., any]] = None,\n    coerces_to_jnp: bool = False,\n    engine: JITEngine = cpu_engine(),\n):\n    \"\"\"\n    Returns a function which JIT compiles the provided function using MAX by first creating a MAX graph,\n    loading it into the MAX engine, and then executing it.\n\n    The first invocation of the JIT'd function will be slow to compile,\n    but subsequent invocations will be fast, as the graph is cached by MAX,\n    and `juju` stores a callable function which avoids repeating\n    the lowering process.\n\n    **Example:**\n\n    ```python exec=\"on\" source=\"material-block\"\n    import jax.numpy as jnp\n    from juju import jit\n\n\n    @jit\n    def foo(x):\n        return x * x\n\n\n    print(foo(5).to_numpy())\n    ```\n\n    `juju.jit` supports an option called `coerces_to_jnp`\n    which can be used to automatically convert MAX tensors\n    to JAX numpy arrays. By default, this option is set to `False`.\n\n    ```python exec=\"on\" source=\"material-block\"\n    import jax.numpy as jnp\n    from juju import jit\n\n\n    @jit(coerces_to_jnp=True)\n    def foo(x):\n        return x * x\n\n\n    print(foo(5))\n    ```\n    \"\"\"\n    if f is None:\n        return functools.partial(\n            jit,\n            coerces_to_jnp=coerces_to_jnp,\n            engine=engine,\n        )\n    return JITFunction(f, coerces_to_jnp, engine)\n</code></pre>"},{"location":"index.html#juju.make_max_graph","title":"juju.make_max_graph","text":"<pre><code>make_max_graph(\n    f: Callable[..., Any],\n) -&gt; Callable[..., Graph]\n</code></pre> <p>Returns a function that constructs and returns a MAX graph for the provided function using JAX tracing.</p> <p>Example:</p> <pre><code>import jax.numpy as jnp\nfrom juju import make_max_graph\n\n\n@make_max_graph\ndef foo(x):\n    return x * x\n\n\nprint(foo(5))\n</code></pre> <p>mo.graph @foo(%arg0: !mo.tensor&lt;[], si32&gt;) -&gt; !mo.tensor&lt;[], si32&gt; attributes {argument_names = [\"input0\"], result_names = [\"output0\"]} {   %0 = mo.chain.create()   %1 = rmo.mul(%arg0, %arg0) : (!mo.tensor&lt;[], si32&gt;, !mo.tensor&lt;[], si32&gt;) -&gt; !mo.tensor&lt;[], si32&gt;   mo.output %1 : !mo.tensor&lt;[], si32&gt; }</p> Source code in <code>src/juju/compiler.py</code> <pre><code>def make_max_graph(f: Callable[..., Any]) -&gt; Callable[..., Graph]:\n    \"\"\"\n    Returns a function that constructs and returns a MAX graph\n    for the provided function using JAX tracing.\n\n    **Example:**\n\n    ```python exec=\"on\" source=\"material-block\"\n    import jax.numpy as jnp\n    from juju import make_max_graph\n\n\n    @make_max_graph\n    def foo(x):\n        return x * x\n\n\n    print(foo(5))\n    ```\n    \"\"\"\n\n    @functools.wraps(f)\n    def wrapped(*args):\n        _, graph = _max(f)(*args)\n        return graph\n\n    return wrapped\n</code></pre>"},{"location":"index.html#custom-operations-and-primitives","title":"Custom operations and primitives","text":"<p>A very nice feature of MAX is that the operation set is extensible, and the language for authoring operations is Mojo, a language with high-level ergonomics (compared to CUDA, for instance).</p> <p>As a result, extending the operation set with new GPU computations is much more approachable than extending XLA with custom CUDA computations, and can be performed without leaving the <code>juju</code> project or introducing external compilers (besides the Mojo compiler, which is accessed via <code>magic</code>).</p> <p>There are two steps to exposing custom operations to <code>juju</code>:</p> <ul> <li>Writing a MAX kernel using Mojo.</li> <li>Exposing the kernel to MAX, and providing the necessary information to JAX in the form of a new <code>Primitive</code>.</li> </ul>"},{"location":"index.html#writing-a-max-kernel","title":"Writing a MAX kernel","text":"<p>A MAX kernel takes the form of a Mojo source code file. The MAX development team has kindly shared several of these kernels for study. Additionally, this article is worth reading to gain a general understanding of custom operations.</p> <p>Let's examine a kernel, and imagine that we've placed this into a folder called <code>kernels/add_one.mojo</code>:</p> kernels/add_one.mojo<pre><code>import compiler\nfrom utils.index import IndexList\nfrom tensor_utils import ManagedTensorSlice, foreach\nfrom runtime.asyncrt import MojoCallContextPtr\n\n\n@compiler.register(\"add_one_custom\", num_dps_outputs=1)\nstruct AddOneCustom:\n    @staticmethod\n    fn execute[\n        # Parameter that if true, runs kernel synchronously in runtime\n        synchronous: Bool,\n        # e.g. \"CUDA\" or \"CPU\"\n        target: StringLiteral,\n    ](\n        # as num_dps_outputs=1, the first argument is the \"output\"\n        out: ManagedTensorSlice,\n        # starting here are the list of inputs\n        x: ManagedTensorSlice[out.type, out.rank],\n        # the context is needed for some GPU calls\n        ctx: MojoCallContextPtr,\n    ):\n        @parameter\n        @always_inline\n        fn func[width: Int](idx: IndexList[x.rank]) -&gt; SIMD[x.type, width]:\n            return x.load[width](idx) + 1\n\n        foreach[func, synchronous, target](out, ctx)\n\n    # You only need to implement this if you do not manually annotate\n    # output shapes in the graph.\n    @staticmethod\n    fn shape(\n        x: ManagedTensorSlice,\n    ) raises -&gt; IndexList[x.rank]:\n        raise \"NotImplemented\"\n</code></pre> <p>Kernels are Mojo structures that are decorated with <code>@compiler.register</code>, and they contain a method called <code>execute</code> which contains the execution semantics of the kernel.</p> <p>To expose the kernel as a MAX operation, the kernel needs to be placed into a Mojo package -- meaning we need an <code>kernels/__init__.mojo</code>:</p> kernels/__init__.mojo<pre><code>from .add_one import *\n</code></pre> <p>We can then ask <code>mojo</code> to compile the Mojo package into a <code>kernels.mojopkg</code>, which we can then use via MAX's Python API to give MAX access to the kernels:</p> <pre><code>mojo package kernels -o kernels.mojopkg\n</code></pre> <p>Keep your kernels package up to date!</p> <p>When implementing custom operations, make sure that the kernels package you're using is up-to-date! Otherwise, during graph loading, MAX will complain about being unable to find your kernel.</p> <p>In the Python API, we can give access to the kernels by providing a <code>custom_extensions</code> argument to <code>engine.InferenceSession</code>:</p> <pre><code>from max import engine \n\nengine.InferenceSession(\n    custom_extensions=\"./kernels.mojopkg\",\n)\n</code></pre> <p>This is exactly how <code>juju</code> does this under the hood, and examining the code should provide further details.</p>"},{"location":"index.html#exposing-the-kernel-to-jax","title":"Exposing the kernel to JAX","text":"<p>Now, MAX is only one side of the coin. The other side is that we'd like to incorporate these computations in JAX source code. </p> <p>JAX allows users to extend JAX's program representations (the Jaxpr) by introducing new primitives, units of computation that accept and return arrays.</p>"},{"location":"index.html#interim-on-the-juju-pipeline","title":"Interim on the <code>juju</code> pipeline","text":"<p><code>juju</code> plugs into JAX in the following way:</p> <ul> <li>(Tracing) First, we use JAX to trace Python computations to produce Jaxprs. </li> <li>(Lowering) Then, <code>juju</code> processes these Jaxprs with an interpreter to create MAX graphs.</li> </ul> <p>Let's say we want to introduce a new primitive to JAX. The first tracing stage requires that the primitive communicate with JAX about the shapes and dtypes of the arrays it accepts as input, as well as the shapes and dtypes of the arrays it produces as output. As long as we tell JAX this information, it doesn't care about \"what the primitive does\". We'll call this information a <code>jax_abstract_evaluation_rule</code>.</p> <p>The second lowering stage requires that we tell the <code>juju</code> interpreter how the primitive is going to be represented in the MAX graph. We'll call this information a <code>max_lowering_rule</code>.</p> <p>To aid in the effort of coordination between JAX and MAX, <code>juju</code> exposes a function called <code>juju.Primitive</code>:</p> <p>For instance, to use our <code>add_one</code> kernel, one would use the following patterns:</p> <pre><code>from juju import Primitive, jit\nfrom jax.core import ShapedArray\nimport jax.numpy as jnp\nfrom max.graph import ops, TensorType\n\n# Lowering rule to MAX, gets called by \n# juju's lowering interpreter.\ndef add_one_lowering(x, **params):\n    return ops.custom(\n        name=\"add_one\",\n        values=[x],\n        out_types=[TensorType(dtype=x.dtype, shape=x.tensor.shape)],\n    )[0]\n\n# Abstract evaluation rule for JAX, gets called\n# by JAX when tracing a program to a Jaxpr.\ndef add_one_abstract(x, **params):\n    return ShapedArray(x.shape, x.dtype)\n\n# Register and coordinate everything, get a callable back.\nadd_one = Primitive(\"add_one\", add_one_lowering, add_one_abstract)\n\n@jit\ndef jaxable_program(x):\n    x = x * 2\n    return add_one(x) # use the callable\n\n# Execute your program using MAX.\nprint(jaxable_program(jnp.ones(10)).to_numpy())\n</code></pre> <p>[3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]</p> <p>The point being that <code>juju.Primitive</code> acts as a very convenient glue between JAX and MAX.</p>"},{"location":"index.html#juju.Primitive","title":"juju.Primitive","text":"<pre><code>Primitive(\n    name: str,\n    max_lowering_rule: Callable,\n    jax_abstract_evaluation_rule: Callable,\n    multiple_results=True,\n)\n</code></pre> <p>Construct a new JAX primitive, and register <code>jax_abstract_evaluation_rule</code> as the abstract evaluation rule for the primitive for JAX, and <code>max_lowering_rule</code> for <code>juju</code>'s lowering interpreter.</p> <p>Returns a function that invokes the primitive via JAX's <code>Primitive.bind</code> method.</p> Source code in <code>src/juju/primitive.py</code> <pre><code>def Primitive(\n    name: str,\n    max_lowering_rule: Callable,\n    jax_abstract_evaluation_rule: Callable,\n    multiple_results=True,\n):\n    \"\"\"\n    Construct a new JAX primitive, and register `jax_abstract_evaluation_rule`\n    as the abstract evaluation rule for the primitive for JAX, and `max_lowering_rule` for `juju`'s lowering interpreter.\n\n    Returns a function that invokes the primitive via JAX's `Primitive.bind` method.\n    \"\"\"\n    new_prim = JPrim(name + \"_p\")\n    new_prim.def_abstract_eval(jax_abstract_evaluation_rule)\n    max_rules.register(new_prim, max_lowering_rule)\n\n    def _raise_impl(*args, **params):\n        raise Exception(f\"{name} is a MAX primitive, cannot be evaluated by JAX.\")\n\n    new_prim.def_impl(_raise_impl)\n\n    def _invoke(*args, **params):\n        return new_prim.bind(*args, **params)\n\n    return _invoke\n</code></pre>"}]}