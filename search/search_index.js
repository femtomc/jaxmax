var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"<code>juju</code>, from JAX to MAX.","text":"<p><code>juju</code> is a bit of compiler middleware bridging (parts of) JAX to the world of MAX graphs. It allows:</p> <ul> <li>users to write JAX programs (see caveat below), lower those programs to MAX graphs, and execute those graphs on MAX-supported hardware, including CPUs, GPUs, and (later on), xPUs (whatever MAX supports).</li> <li>users to extend the Python JAX language (the primitives that JAX exposes to write numerical programs) with custom MAX kernels.</li> </ul> <p>Danger, Will Robinson!</p> <p>This package is a proof-of-concept, and very early in development. Simple programs only for now! It's not yet clear how much of JAX will be fully supported (and how many extensions via MAX kernels will be added).</p> <p>JAX is a massive project, with tons of functionality! It's unlikely that this package will ever support all of JAX (all JAX primitives, and device semantics). The goal is to support enough JAX to be dangerous, and to provide ways to easily extend the functionality of this package to support e.g. more of JAX, or to plug your own custom operations to define your own JAX-like language with compilation to MAX.</p> <p>Example:</p> <p>Using <code>juju</code> to transform and execute code with MAX.</p> <pre><code>import jax.numpy as jnp\nfrom juju import jit\n\n@jit\ndef jax_code(x, y):\n    v = x + y\n    v = v * v\n    return jnp.sin(v)\n\nprint(jax_code(5, 10).to_numpy())\n</code></pre>   -0.93009484"},{"location":"index.html#getting-started","title":"Getting started","text":"<p>To get started with <code>juju</code>, you'll need to follow these steps:</p> <ul> <li>First, install <code>magic</code>, the package and compiler manager for MAX and Mojo.</li> <li>Then, clone this repository, and run <code>magic install</code> at the toplevel. This will setup your environment, which you can access via <code>magic shell</code>. </li> <li>Then, run <code>magic run kernels</code> to build the custom MAX kernels provided as part of <code>juju</code>.</li> </ul>"},{"location":"index.html#basic-apis","title":"Basic APIs","text":"<p>To start out, let's examine basic APIs which allow you to execute functions using MAX, and create MAX graphs. </p>"},{"location":"index.html#juju.jit","title":"juju.jit","text":"<pre><code>jit(\n    f: Optional[Callable[..., any]] = None,\n    coerces_to_jnp: bool = False,\n    engine: JITEngine = cpu_engine(),\n)\n</code></pre> <p>Returns a function which JIT compiles the provided function using MAX by first creating a MAX graph, loading it into the MAX engine, and then executing it.</p> <p>The first invocation of the JIT'd function will be slow to compile, but subsequent invocations will be fast, as the graph is cached by MAX, and <code>juju</code> stores a callable function which avoids repeating the lowering process.</p> <p>Example:</p> <pre><code>import jax.numpy as jnp\nfrom juju import jit\n\n\n@jit\ndef foo(x):\n    return x * x\n\n\nprint(foo(5).to_numpy())\n</code></pre>   25    <p>Automatic conversion of MAX tensors to JAX arrays</p> <p><code>juju.jit</code> supports an option called <code>coerces_to_jnp</code> which can be used to automatically convert MAX tensors to JAX numpy arrays. By default, this option is set to <code>False</code>.</p> <pre><code>import jax.numpy as jnp\nfrom juju import jit\n\n\n@jit(coerces_to_jnp=True)\ndef foo(x):\n    return x * x\n\n\nprint(foo(5))\n</code></pre>   25    <p>Customizing the target platform</p> <p>If you have a GPU available, you can execute the code by using the <code>gpu_engine</code> function to create a JIT engine that uses the GPU.</p> <pre><code>import jax.numpy as jnp\nfrom juju import jit, gpu_engine\n\n\n@jit(engine=gpu_engine())\ndef foo(x):\n    return x * x\n</code></pre> Source code in <code>src/juju/compiler.py</code> <pre><code>def jit(\n    f: Optional[Callable[..., any]] = None,\n    coerces_to_jnp: bool = False,\n    engine: JITEngine = cpu_engine(),\n):\n    \"\"\"\n    Returns a function which JIT compiles the provided function using MAX by first creating a MAX graph,\n    loading it into the MAX engine, and then executing it.\n\n    The first invocation of the JIT'd function will be slow to compile,\n    but subsequent invocations will be fast, as the graph is cached by MAX,\n    and `juju` stores a callable function which avoids repeating\n    the lowering process.\n\n    **Example:**\n\n    ```python exec=\"on\" source=\"material-block\"\n    import jax.numpy as jnp\n    from juju import jit\n\n\n    @jit\n    def foo(x):\n        return x * x\n\n\n    print(foo(5).to_numpy())\n    ```\n\n    **Automatic conversion of MAX tensors to JAX arrays**\n\n    `juju.jit` supports an option called `coerces_to_jnp`\n    which can be used to automatically convert MAX tensors\n    to JAX numpy arrays. By default, this option is set to `False`.\n\n    ```python exec=\"on\" source=\"material-block\"\n    import jax.numpy as jnp\n    from juju import jit\n\n\n    @jit(coerces_to_jnp=True)\n    def foo(x):\n        return x * x\n\n\n    print(foo(5))\n    ```\n\n    **Customizing the target platform**\n\n    If you have a GPU available, you can execute the code by using the\n    `gpu_engine` function to create a JIT engine that uses the GPU.\n\n    ```python\n    import jax.numpy as jnp\n    from juju import jit, gpu_engine\n\n\n    @jit(engine=gpu_engine())\n    def foo(x):\n        return x * x\n    ```\n    \"\"\"\n    if f is None:\n        return functools.partial(\n            jit,\n            coerces_to_jnp=coerces_to_jnp,\n            engine=engine,\n        )\n    return JITFunction(f, coerces_to_jnp, engine)\n</code></pre>"},{"location":"index.html#juju.make_max_graph","title":"juju.make_max_graph","text":"<pre><code>make_max_graph(\n    f: Callable[..., Any],\n) -&gt; Callable[..., Graph]\n</code></pre> <p>Returns a function that constructs and returns a MAX graph for the provided function using JAX tracing.</p> <p>Example:</p> <pre><code>import jax.numpy as jnp\nfrom juju import make_max_graph\n\n\n@make_max_graph\ndef foo(x):\n    return x * x\n\n\nprint(foo(5))\n</code></pre>   mo.graph @foo(%arg0: !mo.tensor&lt;[], si32&gt;) -&gt; !mo.tensor&lt;[], si32&gt; attributes {argument_names = [\"input0\"], result_names = [\"output0\"]} {   %0 = mo.chain.create()   %1 = rmo.mul(%arg0, %arg0) : (!mo.tensor&lt;[], si32&gt;, !mo.tensor&lt;[], si32&gt;) -&gt; !mo.tensor&lt;[], si32&gt;   mo.output %1 : !mo.tensor&lt;[], si32&gt; }    Source code in <code>src/juju/compiler.py</code> <pre><code>def make_max_graph(f: Callable[..., Any]) -&gt; Callable[..., Graph]:\n    \"\"\"\n    Returns a function that constructs and returns a MAX graph\n    for the provided function using JAX tracing.\n\n    **Example:**\n\n    ```python exec=\"on\" source=\"material-block\"\n    import jax.numpy as jnp\n    from juju import make_max_graph\n\n\n    @make_max_graph\n    def foo(x):\n        return x * x\n\n\n    print(foo(5))\n    ```\n    \"\"\"\n\n    @functools.wraps(f)\n    def wrapped(*args):\n        _, graph = _max(f)(*args)\n        return graph\n\n    return wrapped\n</code></pre>"},{"location":"index.html#custom-operations-and-primitives","title":"Custom operations and primitives","text":"<p>A very nice feature of MAX is that the operation set is extensible, and the language for authoring operations is Mojo, a language with high-level ergonomics (compared to CUDA, for instance).</p> <p>As a result, extending the operation set with new GPU computations is much more approachable than extending XLA with custom CUDA computations, and can be performed without leaving the <code>juju</code> project or introducing external compilers (besides the Mojo compiler, which is accessed via <code>magic</code>).</p> <p>There are two steps to exposing custom operations to <code>juju</code>:</p> <ul> <li>Writing a MAX kernel using Mojo.</li> <li>Exposing the kernel to MAX, and providing the necessary information to JAX in the form of a new <code>Primitive</code>.</li> </ul>"},{"location":"index.html#writing-a-max-kernel","title":"Writing a MAX kernel","text":"<p>A MAX kernel takes the form of a Mojo source code file. The MAX development team has kindly shared several of these kernels for study. Additionally, this article is worth reading to gain a general understanding of custom operations.</p> <p>Let's examine a kernel, and imagine that we've placed this into a folder called <code>kernels/add_one.mojo</code>:</p> kernels/add_one.mojo<pre><code>import compiler\nfrom utils.index import IndexList\nfrom max.tensor import ManagedTensorSlice, foreach\nfrom runtime.asyncrt import MojoCallContextPtr\n\n\n@compiler.register(\"add_one\", num_dps_outputs=1)\nstruct AddOneCustom:\n    @staticmethod\n    fn execute[\n        # Parameter that if true, runs kernel synchronously in runtime\n        synchronous: Bool,\n        # e.g. \"CUDA\" or \"CPU\"\n        target: StringLiteral,\n    ](\n        # as num_dps_outputs=1, the first argument is the \"output\"\n        out: ManagedTensorSlice,\n        # starting here are the list of inputs\n        x: ManagedTensorSlice[out.type, out.rank],\n        # the context is needed for some GPU calls\n        ctx: MojoCallContextPtr,\n    ):\n        @parameter\n        @always_inline\n        fn func[width: Int](idx: IndexList[x.rank]) -&gt; SIMD[x.type, width]:\n            return x.load[width](idx) + 1\n\n        foreach[func, synchronous, target](out, ctx)\n\n    # You only need to implement this if you do not manually annotate\n    # output shapes in the graph.\n    @staticmethod\n    fn shape(\n        x: ManagedTensorSlice,\n    ) raises -&gt; IndexList[x.rank]:\n        raise \"NotImplemented\"\n</code></pre> <p>Kernels are Mojo structures that are decorated with <code>@compiler.register</code>, and they contain a method called <code>execute</code> which contains the execution semantics of the kernel.</p> <p>To expose the kernel as a MAX operation, the kernel needs to be placed into a Mojo package -- meaning we need a <code>kernels/__init__.mojo</code>:</p> kernels/__init__.mojo<pre><code>from .add_one import *\n</code></pre> <p>We can then ask <code>mojo</code> to compile the Mojo package into a <code>kernels.mojopkg</code>, which we can then use via MAX's Python API to give MAX access to the kernels:</p> <pre><code>mojo package kernels -o kernels.mojopkg\n</code></pre> <p>Keep your kernels package up to date!</p> <p>When implementing custom operations, make sure that the kernels package you're using is up-to-date! Otherwise, during graph loading, MAX will complain about being unable to find your kernel.</p> <p>In the Python API, we can give access to the kernels by providing a <code>custom_extensions</code> argument to <code>engine.InferenceSession</code>:</p> <pre><code>from max import engine \n\nengine.InferenceSession(\n    custom_extensions=\"./kernels.mojopkg\",\n)\n</code></pre> <p>This is exactly how <code>juju</code> does this under the hood, and examining the code should provide further details.</p>"},{"location":"index.html#exposing-the-kernel-to-jax","title":"Exposing the kernel to JAX","text":"<p>Now, MAX is only one side of the coin. The other side is that we'd like to incorporate these computations in JAX source code. </p> <p>JAX allows users to extend JAX's program representations (the Jaxpr) by introducing new primitives, units of computation that accept and return arrays.</p>"},{"location":"index.html#interim-on-the-juju-pipeline","title":"Interim on the <code>juju</code> pipeline","text":"<p><code>juju</code> plugs into JAX in the following way:</p> <ul> <li>(Tracing) First, we use JAX to trace Python computations to produce Jaxprs. </li> <li>(Lowering) Then, <code>juju</code> processes these Jaxprs with an interpreter to create MAX graphs.</li> </ul> <p>Let's say we want to introduce a new primitive to JAX. The first tracing stage requires that the primitive communicate with JAX about the shapes and dtypes of the arrays it accepts as input, as well as the shapes and dtypes of the arrays it produces as output. As long as we tell JAX this information, it doesn't care about \"what the primitive does\". We'll call this information a <code>jax_abstract_evaluation_rule</code>.</p> <p>The second lowering stage requires that we tell the <code>juju</code> interpreter how the primitive is going to be represented in the MAX graph. We'll call this information a <code>max_lowering_rule</code>.</p> <p>To aid in the effort of coordination between JAX and MAX, <code>juju</code> exposes a function called <code>juju.Primitive</code>:</p> <p>For instance, to use our <code>add_one</code> kernel, one would use the following patterns:</p> using_our_prim.py<pre><code>from juju import Primitive, jit\nfrom jax.core import ShapedArray\nimport jax.numpy as jnp\nfrom max.graph import ops, TensorType\n\n# Lowering rule to MAX, gets called by \n# juju's lowering interpreter.\ndef add_one_lowering(x, **params):\n    return ops.custom(\n        name=\"add_one\", # needs to match your @compiler.register name\n        values=[x],\n        out_types=[TensorType(dtype=x.dtype, shape=x.tensor.shape)],\n    )[0]\n\n# Abstract evaluation rule for JAX, gets called\n# by JAX when tracing a program to a Jaxpr.\ndef add_one_abstract(x, **params):\n    return ShapedArray(x.shape, x.dtype)\n\n# Register and coordinate everything, get a callable back.\nadd_one = Primitive(\n    \"add_one\", # can be anything\n    add_one_lowering, \n    add_one_abstract,\n)\n\n@jit\ndef jaxable_program(x):\n    x = x * 2\n    return add_one(x) # use the callable\n\n# Execute your program using MAX.\nprint(jaxable_program(jnp.ones(10)).to_numpy())\n</code></pre>   [3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]    <p>The point being that <code>juju.Primitive</code> acts as a very convenient glue between JAX and MAX.</p>"},{"location":"index.html#juju.Primitive","title":"juju.Primitive","text":"<pre><code>Primitive(\n    name: str,\n    max_lowering_rule: Callable,\n    jax_abstract_evaluation_rule: Callable,\n    multiple_results=True,\n)\n</code></pre> <p>Construct a new JAX primitive, and register <code>jax_abstract_evaluation_rule</code> as the abstract evaluation rule for the primitive for JAX, and <code>max_lowering_rule</code> for <code>juju</code>'s lowering interpreter.</p> <p>Returns a function that invokes the primitive via JAX's <code>Primitive.bind</code> method.</p> Source code in <code>src/juju/primitive.py</code> <pre><code>def Primitive(\n    name: str,\n    max_lowering_rule: Callable,\n    jax_abstract_evaluation_rule: Callable,\n    multiple_results=True,\n):\n    \"\"\"\n    Construct a new JAX primitive, and register `jax_abstract_evaluation_rule`\n    as the abstract evaluation rule for the primitive for JAX, and `max_lowering_rule` for `juju`'s lowering interpreter.\n\n    Returns a function that invokes the primitive via JAX's `Primitive.bind` method.\n    \"\"\"\n    new_prim = JPrim(name)\n    new_prim.def_abstract_eval(jax_abstract_evaluation_rule)\n    max_rules.register(new_prim, max_lowering_rule)\n\n    # JAX can't execute the code by itself!\n    # We have to use MAX, so we raise an exception if JAX tries to evaluate the primitive.\n    def _raise_impl(*args, **params):\n        raise Exception(f\"{name} is a MAX primitive, cannot be evaluated by JAX.\")\n\n    new_prim.def_impl(_raise_impl)\n\n    def _invoke(*args, **params):\n        return new_prim.bind(*args, **params)\n\n    return _invoke\n</code></pre>"},{"location":"lowering_rules.html","title":"Lowering rules","text":"<p>Here, we list the set of JAX primitives with MAX lowering rules. These rules are used internally by <code>juju</code>'s lowering interpreter, and this list reflects the coverage of lowering over JAX's primitives.</p> <p>Note that, even if a JAX primitive is in the list below, it's possible that our semantics are incorrect or missing some configuration that JAX supports. Our test suite is the place where we test fidelity of the lowering, so if something appears to be misbehaving, please file an issue. If you're using <code>juju</code> and we're missing a lowering rule, also please file an issue!</p> <pre><code>from juju.rules import max_rules\n\nfor primitive in list(max_rules.keys()):\n    print(primitive)\n</code></pre>   add mul sub sin cos abs max min exp log floor acos iota div integer_pow reduce_sum neg add_any convert_element_type reshape broadcast_in_dim concatenate pjit add_one    <p>The implementation of these rules can be found in the <code>juju.rules</code> module.</p>"},{"location":"pseudorandomness.html","title":"Pseudorandomness","text":"<p>Check back later!</p>"}]}